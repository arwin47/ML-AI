{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17547622",
   "metadata": {},
   "source": [
    "### Tweets Sentiment Prediction using NLP and Classification algorithm\n",
    "\n",
    "The goal is to predict if a review has got negative or positive sentiment aiding faster decision by the reader or the business in classifying reviews and make decisions basis the sentiment. Finding the sentiment also aids in summarizing the outcome for perfoemance measurement which otherwise becomes highly manual process.\n",
    "\n",
    "\n",
    "#### Dataset:\n",
    "The dataset contains tweets and associated sentiments tagged.\n",
    "Source: https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c426fc",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "955a7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(r\"Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19202ecc",
   "metadata": {},
   "source": [
    "#### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a163d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c786703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     11118\n",
       "positive     8582\n",
       "negative     7781\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56260f11",
   "metadata": {},
   "source": [
    "Data looks well balanced and we'll use only the 'text' and 'sentiment' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84f4c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c31bb44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0                I`d have responded, if I were going   neutral\n",
       "1      Sooo SAD I will miss you here in San Diego!!!  negative\n",
       "2                          my boss is bullying me...  negative\n",
       "3                     what interview! leave me alone  negative\n",
       "4   Sons of ****, why couldn`t they put them on t...  negative"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d505b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612fa24",
   "metadata": {},
   "source": [
    "Resetting the index to remove outdated indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6785c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index() # to remove outdated indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "839d6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "651b5ce9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0                I`d have responded, if I were going   neutral\n",
       "1      Sooo SAD I will miss you here in San Diego!!!  negative\n",
       "2                          my boss is bullying me...  negative\n",
       "3                     what interview! leave me alone  negative\n",
       "4   Sons of ****, why couldn`t they put them on t...  negative"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61975fc",
   "metadata": {},
   "source": [
    "Encoding target feature 'sentiment' before feeding to classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff53ce37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "data['sentiment']= le.fit_transform(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47c7b9c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8722caa",
   "metadata": {},
   "source": [
    "Here we have three classes encoded as 0,1,2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e3613",
   "metadata": {},
   "source": [
    "Functionalities and explanations for each cleaning step will be added as a part of another detailed readout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5085826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Libraries for cleaning text data\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') # To identify stop words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer  # To stem the word to it's basic form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3d505",
   "metadata": {},
   "source": [
    "In this section, we will look at only positive and negative reviews to do a binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20050eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for i in range(0, len(data)):\n",
    "  tweet = re.sub('[^a-zA-Z]', ' ', data['text'][i])  # Using only alphabets\n",
    "  tweet = tweet.lower()   \n",
    "  tweet = tweet.split()\n",
    "  ps = PorterStemmer()\n",
    "  all_stopwords = stopwords.words('english')   # using English stopwords\n",
    "  all_stopwords.remove('not')\n",
    "  tweet = [ps.stem(word) for word in tweet if not word in set(all_stopwords)] # stemming all words that are not stop words\n",
    "  tweet = ' '.join(tweet)\n",
    "  word_list.append(tweet)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e7242ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sooo sad miss san diego',\n",
       " 'boss bulli',\n",
       " 'interview leav alon',\n",
       " 'son put releas alreadi bought']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4311b15c",
   "metadata": {},
   "source": [
    "Model can be improved by better methods available as we see they are not done as expected. \n",
    "Let us use lemmatizing which is a better and advanced way of reducing words to it's basic form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c4b7ff",
   "metadata": {},
   "source": [
    "Downloading required resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927acf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c474b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "# Create WordNetLemmatizer object\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f378d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for i in range(0, len(data)):\n",
    "  tweet = re.sub('[^a-zA-Z]', ' ', data['text'][i])  # Using only alphabets\n",
    "  tweet = tweet.lower()   \n",
    "  tweet = tweet.split()\n",
    "  all_stopwords = stopwords.words('english')   # using English stopwords\n",
    "  all_stopwords.remove('not')\n",
    "  tweet = [wnl.lemmatize(word) for word in tweet if not word in set(all_stopwords)] # lemmatizing all words that are not stop words\n",
    "  tweet = ' '.join(tweet)\n",
    "  word_list.append(tweet)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "418621fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sooo sad miss san diego',\n",
       " 'bos bullying',\n",
       " 'interview leave alone',\n",
       " 'son put release already bought']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9df311",
   "metadata": {},
   "source": [
    "Lemmatized list is much better but still not great. Will use this list to train the model and understand the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a028364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(word_list).toarray()\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41197ed7",
   "metadata": {},
   "source": [
    "Checking the size of the X vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f02ddab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22536"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2021d236",
   "metadata": {},
   "source": [
    "Considering only 15000 frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14cd6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features= 15000)\n",
    "X = vectorizer.fit_transform(word_list).toarray()\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191720b",
   "metadata": {},
   "source": [
    "### Splitting dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ac0af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ea4be",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c872f",
   "metadata": {},
   "source": [
    "Using a Multinomial Naive Bayes classifier for classifying the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1957acc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575718de",
   "metadata": {},
   "source": [
    "### Testing with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "445f2531",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2]\n",
      " [2 1]\n",
      " [2 2]\n",
      " ...\n",
      " [1 0]\n",
      " [2 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a03ef",
   "metadata": {},
   "source": [
    "### Measuring accuracy and making confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1ff4fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 912  501  110]\n",
      " [ 372 1484  419]\n",
      " [  83  455 1160]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6470160116448326"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcafdd37",
   "metadata": {},
   "source": [
    "Now limiting the vector to 12000 words to see if performance on test data increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "faba4ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features= 12000)\n",
    "X = vectorizer.fit_transform(word_list).toarray()\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a7aa119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "682e6342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 926  488  109]\n",
      " [ 381 1473  421]\n",
      " [  86  452 1160]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6475618631732168"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "# print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c53f578",
   "metadata": {},
   "source": [
    "Not a lot of improvement in accuracy. Trying with more limited words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "649f0d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 873  554   96]\n",
      " [ 322 1608  345]\n",
      " [  79  488 1131]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6572052401746725"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features= 1000)\n",
    "X = vectorizer.fit_transform(word_list).toarray()\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "# print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e20389",
   "metadata": {},
   "source": [
    "By going with 10000 words vector, the accuracy that we achieved is 66%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
